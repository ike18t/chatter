# Voice AI Assistant

A Python-based voice AI assistant with push-to-talk functionality that combines speech recognition, AI processing, and text-to-speech synthesis.

## Features

- ğŸ™ï¸ **Push-to-talk interface** using Gradio web UI
- ğŸ—£ï¸ **Speech-to-text** via local OpenAI Whisper
- ğŸ¤– **AI processing** with Ollama DeepSeek-R1
- ğŸ”Š **Text-to-speech** using ChatTTS
- ğŸŒ **Web-based interface** accessible from any browser

## Architecture

```
Audio Input â†’ OpenAI Whisper â†’ DeepSeek-R1 â†’ ChatTTS â†’ Audio Output
```

1. **Recording**: Push-to-talk captures audio input
2. **Transcription**: Local OpenAI Whisper converts speech to text
3. **Processing**: Ollama DeepSeek-R1 generates intelligent responses
4. **Synthesis**: ChatTTS converts response back to speech

## Prerequisites

- Python 3.13+
- [Ollama](https://ollama.ai/download) installed and running
- Audio input/output devices

## Installation

1. **Clone and setup the project:**
   ```bash
   cd voice-ai-assistant
   uv sync
   ```

2. **Run the application (models download automatically):**
   ```bash
   uv run voice-assistant
   ```

   Or alternatively:
   ```bash
   uv run python -m src.voice_ai_assistant.main
   ```

On first run, the following models will be downloaded automatically:
- `deepseek-r1:7b` for AI responses (via Ollama)
- ChatTTS models for text-to-speech
- OpenAI Whisper tiny model for speech recognition

## Usage

1. **Start the application:**
   ```bash
   uv run voice-assistant
   ```

2. **Open your browser** to `http://localhost:7860`

3. **Use the interface:**
   - Click "ğŸ¤ Start Recording" to begin recording
   - Speak your message
   - Click "â¹ï¸ Stop & Process" to process and get AI response
   - Listen to the generated audio response

## Project Structure

```
voice-ai-assistant/
â”œâ”€â”€ src/voice_ai_assistant/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py              # Main application logic
â”œâ”€â”€ install_models.py        # Ollama model installation script
â”œâ”€â”€ run.py                   # Simple runner script
â”œâ”€â”€ pyproject.toml          # Project configuration
â””â”€â”€ README.md
```

## Dependencies

- **gradio**: Web-based UI framework
- **ollama**: Interface to Ollama models
- **chattts**: Text-to-speech synthesis
- **sounddevice**: Audio recording
- **numpy**: Audio processing
- **openai-whisper**: Speech recognition
- **torchaudio**: Audio file handling

## Configuration

The application uses default settings:
- **Sample rate**: 16kHz
- **Channels**: Mono
- **Server**: localhost:7860

## Troubleshooting

**Audio issues:**
- Ensure microphone permissions are granted
- Check audio device availability with `python -c "import sounddevice; print(sounddevice.query_devices())"`

**Ollama connection:**
- Verify Ollama is running: `ollama list`
- Ensure models are installed: `ollama pull deepseek-r1:32b`

**Performance:**
- First run may be slower due to model loading
- ChatTTS initialization takes a few seconds on startup
- Whisper model will be downloaded automatically on first use

## License

MIT License